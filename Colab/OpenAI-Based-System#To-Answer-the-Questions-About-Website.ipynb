{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPsZQLh9Y3/+8gkWWqCdrFJ"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["def remove_newlines(serie):\n","    serie = serie.str.replace('\\n', ' ')\n","    serie = serie.str.replace('\\\\n', ' ')\n","    serie = serie.str.replace('  ', ' ')\n","    serie = serie.str.replace('  ', ' ')\n","    return serie"],"metadata":{"id":"Q-ldKajVtOEI"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Converting the text to CSV requires looping through the text files in the text directory created earlier. After opening each file, remove the extra spacing and append the modified text to a list. Then, add the text with the new lines removed to an empty Pandas data frame and write the data frame to a CSV file."],"metadata":{"id":"ezWnV6agtv58"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ijg8SiDds9Os"},"outputs":[],"source":["import pandas as pd\n","\n","# Create a list to store the text files\n","texts=[]\n","\n","# Get all the text files in the text directory\n","for file in os.listdir(\"text/\" + domain + \"/\"):\n","\n","    # Open the file and read the text\n","    with open(\"text/\" + domain + \"/\" + file, \"r\", encoding=\"UTF-8\") as f:\n","        text = f.read()\n","\n","        # Omit the first 11 lines and the last 4 lines, then replace -, _, and #update with spaces.\n","        texts.append((file[11:-4].replace('-',' ').replace('_', ' ').replace('#update',''), text))\n","\n","# Create a dataframe from the list of texts\n","df = pd.DataFrame(texts, columns = ['fname', 'text'])\n","\n","# Set the text column to be the raw text with the newlines removed\n","df['text'] = df.fname + \". \" + remove_newlines(df.text)\n","df.to_csv('processed/scraped.csv')\n","df.head()"]},{"cell_type":"markdown","source":["A helpful rule of thumb is that one token generally corresponds to ~4 characters of text for common English text. This translates to roughly Â¾ of a word (so 100 tokens ~= 75 words)."],"metadata":{"id":"PrynCVypuJ_z"}},{"cell_type":"code","source":["import tiktoken\n","\n","# Load the cl100k_base tokenizer which is designed to work with the ada-002 model\n","tokenizer = tiktoken.get_encoding(\"cl100k_base\")\n","\n","df = pd.read_csv('processed/scraped.csv', index_col=0)\n","df.columns = ['title', 'text']\n","\n","# Tokenize the text and save the number of tokens to a new column\n","df['n_tokens'] = df.text.apply(lambda x: len(tokenizer.encode(x)))\n","\n","# Visualize the distribution of the number of tokens per row using a histogram\n","df.n_tokens.hist()"],"metadata":{"id":"qBkdxhSOtz2d"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["max_tokens = 500\n","\n","# Function to split the text into chunks of a maximum number of tokens\n","def split_into_many(text, max_tokens = max_tokens):\n","\n","    # Split the text into sentences\n","    sentences = text.split('. ')\n","\n","    # Get the number of tokens for each sentence\n","    n_tokens = [len(tokenizer.encode(\" \" + sentence)) for sentence in sentences]\n","    \n","    chunks = []\n","    tokens_so_far = 0\n","    chunk = []\n","\n","    # Loop through the sentences and tokens joined together in a tuple\n","    for sentence, token in zip(sentences, n_tokens):\n","\n","        # If the number of tokens so far plus the number of tokens in the current sentence is greater \n","        # than the max number of tokens, then add the chunk to the list of chunks and reset\n","        # the chunk and tokens so far\n","        if tokens_so_far + token > max_tokens:\n","            chunks.append(\". \".join(chunk) + \".\")\n","            chunk = []\n","            tokens_so_far = 0\n","\n","        # If the number of tokens in the current sentence is greater than the max number of \n","        # tokens, go to the next sentence\n","        if token > max_tokens:\n","            continue\n","\n","        # Otherwise, add the sentence to the chunk and add the number of tokens to the total\n","        chunk.append(sentence)\n","        tokens_so_far += token + 1\n","\n","    return chunks\n","    \n","\n","shortened = []\n","\n","# Loop through the dataframe\n","for row in df.iterrows():\n","\n","    # If the text is None, go to the next row\n","    if row[1]['text'] is None:\n","        continue\n","\n","    # If the number of tokens is greater than the max number of tokens, split the text into chunks\n","    if row[1]['n_tokens'] > max_tokens:\n","        shortened += split_into_many(row[1]['text'])\n","    \n","    # Otherwise, add the text to the list of shortened texts\n","    else:\n","        shortened.append( row[1]['text'] )"],"metadata":{"id":"2iMN-6ywuTDx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df = pd.DataFrame(shortened, columns = ['text'])\n","df['n_tokens'] = df.text.apply(lambda x: len(tokenizer.encode(x)))\n","df.n_tokens.hist()"],"metadata":{"id":"gglID2s_u2JE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import openai\n","\n","df['embeddings'] = df.text.apply(lambda x: openai.Embedding.create(input=x, engine='text-embedding-ada-002')['data'][0]['embedding'])\n","\n","df.to_csv('processed/embeddings.csv')\n","df.head()"],"metadata":{"id":"Nz4_wpMmvdUi"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Building a question answer system with your embeddings"],"metadata":{"id":"ZoiHykx2v3Kv"}},{"cell_type":"code","source":["import numpy as np\n","from openai.embeddings_utils import distances_from_embeddings\n","\n","df=pd.read_csv('processed/embeddings.csv', index_col=0)\n","df['embeddings'] = df['embeddings'].apply(eval).apply(np.array)\n","\n","df.head()"],"metadata":{"id":"FOvhCd4cv4nU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def create_context(\n","    question, df, max_len=1800, size=\"ada\"):\n","    \"\"\"\n","    Create a context for a question by finding the most similar context from the dataframe\n","    \"\"\"\n","\n","    # Get the embeddings for the question\n","    q_embeddings = openai.Embedding.create(input=question, engine='text-embedding-ada-002')['data'][0]['embedding']\n","\n","    # Get the distances from the embeddings\n","    df['distances'] = distances_from_embeddings(q_embeddings, df['embeddings'].values, distance_metric='cosine')\n","\n","\n","    returns = []\n","    cur_len = 0\n","\n","    # Sort by distance and add the text to the context until the context is too long\n","    for i, row in df.sort_values('distances', ascending=True).iterrows():\n","        \n","        # Add the length of the text to the current length\n","        cur_len += row['n_tokens'] + 4\n","        \n","        # If the context is too long, break\n","        if cur_len > max_len:\n","            break\n","        \n","        # Else add it to the text that is being returned\n","        returns.append(row[\"text\"])\n","\n","    # Return the context\n","    return \"\\n\\n###\\n\\n\".join(returns)"],"metadata":{"id":"9chFzoKewKdO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def answer_question(\n","    df,\n","    model=\"text-davinci-003\",\n","    question=\"Am I allowed to publish model outputs to Twitter, without a human review?\",\n","    max_len=1800,\n","    size=\"ada\",\n","    debug=False,\n","    max_tokens=150,\n","    stop_sequence=None):\n","  \n","    \"\"\"\n","    Answer a question based on the most similar context from the dataframe texts\n","    \"\"\"\n","    context = create_context(\n","        question,\n","        df,\n","        max_len=max_len,\n","        size=size,\n","    )\n","    # If debug, print the raw model response\n","    if debug:\n","        print(\"Context:\\n\" + context)\n","        print(\"\\n\\n\")\n","\n","    try:\n","        # Create a completions using the question and context\n","        response = openai.Completion.create(\n","            prompt=f\"Answer the question based on the context below, and if the question can't be answered based on the context, say \\\"I don't know\\\"\\n\\nContext: {context}\\n\\n---\\n\\nQuestion: {question}\\nAnswer:\",\n","            temperature=0,\n","            max_tokens=max_tokens,\n","            top_p=1,\n","            frequency_penalty=0,\n","            presence_penalty=0,\n","            stop=stop_sequence,\n","            model=model,\n","        )\n","        return response[\"choices\"][0][\"text\"].strip()\n","    except Exception as e:\n","        print(e)\n","        return \"\""],"metadata":{"id":"P1BwHKNlwV8y"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["answer_question(df, question=\"What day is it?\", debug=False)\n","\n","answer_question(df, question=\"What is our newest embeddings model?\")\n","\n","answer_question(df, question=\"What is ChatGPT?\")"],"metadata":{"id":"d1sU8zwRwbTA"},"execution_count":null,"outputs":[]}]}